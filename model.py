import torch
import torch.nn as nn
import torch.nn.functional as F

'''
class MalConv(nn.Module):
    def __init__(self, max_len, embedding_dim, kernel_size):
        super(MalConv, self).__init__()
        self.tok_embedding = nn.Embedding(257, embedding_dim)
        self.pos_embedding = nn.Embedding(max_len, embedding_dim)
        self.dropout = nn.Dropout(p=0.5)
        self.conv = nn.Conv1d(in_channels=embedding_dim,
                              out_channels=128 * 2,
                              kernel_size=kernel_size,
                              stride=kernel_size)
        self.fc = nn.Linear(128, 1)

    def forward(self, x):
        tok = self.tok_embedding(x)
        batch_size, max_len = x.shape[0], x.shape[1]
        pos = torch.arange(max_len).unsqueeze(0).repeat(batch_size, 1)
        pos = pos.to(tok.device)
        pos = self.pos_embedding(pos)
        out = self.dropout(tok + pos)
        out = out.permute(0, 2, 1)
        out = self.conv(out)
        out = F.glu(out, dim=1)
        out, _ = out.max(dim=-1)
        return self.fc(out).squeeze(1)
'''


class MalConv(nn.Module):
    def __init__(self, max_len, window_size, num_embeddings=257, embedding_dim=8, num_channels=128):
        super(MalConv, self).__init__()
        self.tok_embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.pos_embedding = nn.Embedding(max_len, embedding_dim)
        self.dropout = nn.Dropout(p=0.5)
        self.conv1 = nn.Conv1d(embedding_dim, num_channels, kernel_size=window_size, stride=window_size)
        self.conv2 = nn.Conv1d(embedding_dim, num_channels, kernel_size=window_size, stride=window_size)
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.fc1 = nn.Linear(num_channels, num_channels)
        self.fc2 = nn.Linear(num_channels, 2)

    def forward(self, x):
        # embedding layers
        tok = self.tok_embedding(x)
        pos = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0], 1).to(tok.device)
        pos = self.pos_embedding(pos)
        out_embed = self.dropout(tok + pos).permute(0, 2, 1)

        # convolution layers
        out_conv1 = self.conv1(out_embed)
        out_conv2 = torch.sigmoid(self.conv2(out_embed))
        out = out_conv1 * out_conv2

        # pooling layer
        out = self.pool(out).squeeze(2)

        # full connected layers
        out = F.relu(self.fc1(out))
        return self.fc2(out)
