import torch
import torch.nn as nn
import torch.nn.functional as F


class MalConv(nn.Module):
    def __init__(self, max_len, embedding_dim, kernel_size):
        super(MalConv, self).__init__()
        self.tok_embedding = nn.Embedding(257, embedding_dim)
        self.pos_embedding = nn.Embedding(max_len, embedding_dim)
        self.dropout = nn.Dropout(p=0.5)
        self.conv = nn.Conv1d(in_channels=embedding_dim,
                              out_channels=128 * 2,
                              kernel_size=kernel_size,
                              stride=kernel_size)
        self.fc = nn.Linear(128, 1)

    def forward(self, x):
        tok = self.tok_embedding(x)
        batch_size, max_len = x.shape[0], x.shape[1]
        pos = torch.arange(max_len).unsqueeze(0).repeat(batch_size, 1)
        pos = pos.to(tok.device)
        pos = self.pos_embedding(pos)
        out = self.dropout(tok + pos)
        out = out.permute(0, 2, 1)
        out = self.conv(out)
        out = F.glu(out, dim=1)
        out, _ = out.max(dim=-1)
        return self.fc(out).squeeze(1)
