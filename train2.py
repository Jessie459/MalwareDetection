import argparse
import os
import shutil

import torch
import torch.cuda.amp
from torch import optim
from torch.optim.lr_scheduler import ExponentialLR

from dataset import MalwareDataset, create_loaders
from model import MalConv
from utils import plot_curve, set_seed

parser = argparse.ArgumentParser(description='Train the MalConv model.')
parser.add_argument('--device', type=str, default='cuda:0')
parser.add_argument('--max_len', type=int, default=1024 * 1024)
parser.add_argument('--window_size', type=int, default=512)
parser.add_argument('--batch_size', type=int, default=64)
parser.add_argument('--num_epochs', type=int, default=20)
parser.add_argument('--lr', type=float, default=0.001)
parser.add_argument('--internal', type=int, default=50)
parser.add_argument('--resume', action='store_true')
args = parser.parse_args()

device = torch.device(args.device)
scaler = torch.cuda.amp.GradScaler()


def train(model, train_loader, valid_loader, lr, num_epochs, resume=False):
    """
    Train the MalConv model.
    """
    model_path = os.path.join('results', 'models', 'model.pth')
    model_min_loss_path = os.path.join('results', 'models', 'model_min_loss.pth')
    model_max_accu_path = os.path.join('results', 'models', 'model_max_accu.pth')

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = ExponentialLR(optimizer, gamma=0.9)

    if resume:
        state_dict = torch.load(model_path)
        model.load_state_dict(state_dict['model'])
        optimizer.load_state_dict(state_dict['optimizer'])
        start_epoch = state_dict['epoch'] + 1
        min_loss = state_dict['loss']
        max_accu = state_dict['accu']
    else:
        start_epoch = 1
        min_loss = float('inf')
        max_accu = 0

    train_epoch_loss = []
    train_epoch_accu = []
    train_batch_loss = []
    train_batch_accu = []

    valid_epoch_loss = []
    valid_epoch_accu = []
    valid_batch_loss = []
    valid_batch_accu = []

    for epoch in range(start_epoch, num_epochs + 1):
        print(f'Epoch: [{epoch}|{num_epochs}]')

        # train the model
        print(f'Training')
        model.train()
        train_history = run_epoch(model, train_loader, criterion, optimizer)
        train_epoch_loss.append(train_history[0])
        train_epoch_accu.append(train_history[1])
        train_batch_loss += train_history[2]
        train_batch_accu += train_history[3]

        # validate the model
        print(f'Validating')
        model.eval()
        with torch.no_grad():
            valid_history = run_epoch(model, valid_loader, criterion)
        valid_epoch_loss.append(valid_history[0])
        valid_epoch_accu.append(valid_history[1])
        valid_batch_loss += valid_history[2]
        valid_batch_accu += valid_history[3]

        # adjust the learning rate
        scheduler.step()

        # save the model
        valid_loss = valid_history[0]
        valid_accu = valid_history[1]
        torch.save({'model': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'epoch': epoch,
                    'loss': valid_loss,
                    'accu': valid_accu}, model_path)
        if min_loss > valid_loss:
            shutil.copyfile(model_path, model_min_loss_path)
            min_loss = valid_loss
        if max_accu < valid_accu:
            shutil.copyfile(model_path, model_max_accu_path)
            max_accu = valid_accu

    plot_curve(train_batch_loss, train_batch_accu,
               label1='loss', label2='accu',
               x_label='batch', y_label='loss and accu',
               title='train loss and accu (batch)', save_name='batch_train')
    plot_curve(valid_batch_loss, valid_batch_accu,
               label1='loss', label2='accu',
               x_label='batch', y_label='loss and accu',
               title='valid loss and accu (batch)', save_name='batch_valid')
    plot_curve(train_epoch_loss, valid_epoch_loss,
               label1='train loss', label2='valid loss',
               x_label='epoch', y_label='loss',
               title='train and valid loss (epoch)', save_name='epoch_loss')
    plot_curve(train_epoch_accu, valid_epoch_accu,
               label1='train accu', label2='valid accu',
               x_label='epoch', y_label='accu',
               title='train and valid accu (epoch)', save_name='epoch_accu')


def run_epoch(model, loader, criterion, optimizer=None):
    epoch_loss = 0
    epoch_accu = 0
    batch_loss = []
    batch_accu = []

    for index, (inputs, labels) in enumerate(loader):
        inputs = inputs.to(device)
        labels = labels.to(device)

        with torch.cuda.amp.autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        if optimizer:
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

        loss_item = loss.item()
        accu_item = (torch.argmax(outputs, dim=1) == labels).sum().item()
        epoch_loss += loss_item
        epoch_accu += accu_item

        if (index + 1) % args.internal == 0:
            accu_item /= inputs.shape[0]
            batch_loss.append(loss_item)
            batch_accu.append(accu_item)
            print(f'Batch: [{index + 1:3d}|{len(loader):3d}] '
                  f'Loss: {loss_item:.4f} '
                  f'Accu: {accu_item:.4f}')

    epoch_loss /= len(loader)
    epoch_accu /= len(loader.dataset)
    print(f'Epoch Loss: {epoch_loss:.4f} '
          f'Epoch Accu: {epoch_accu:.4f}')

    return epoch_loss, epoch_accu, batch_loss, batch_accu


def main():
    set_seed(1)
    print('Running on device ' + args.device)

    os.makedirs(os.path.join('results', 'images'), exist_ok=True)
    os.makedirs(os.path.join('results', 'models'), exist_ok=True)

    dataset = MalwareDataset(max_len=args.max_len)
    train_loader, valid_loader = create_loaders(dataset, batch_size=args.batch_size)
    model = MalConv(window_size=args.window_size)
    model = model.to(device)

    train(model, train_loader, valid_loader, lr=args.lr,
          num_epochs=args.num_epochs, resume=args.resume)


if __name__ == '__main__':
    main()
